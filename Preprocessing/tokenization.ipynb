{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54224806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8b707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9b20f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/bisakhpatra/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1e109f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Hello there! I'm testing different tokenizers—let's see how they work.Some tokenizers split contractions like \\\"don't\\\" or \\\"it's\\\", while others don't.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5b943fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: \n",
      "['Hello there!', 'I\\'m testing different tokenizers—let\\'s see how they work.Some tokenizers split contractions like \"don\\'t\" or \"it\\'s\", while others don\\'t.']\n",
      "------------------------------\n",
      "Word Tokenization: \n",
      "['Hello', 'there', '!', 'I', \"'m\", 'testing', 'different', 'tokenizers—let', \"'s\", 'see', 'how', 'they', 'work.Some', 'tokenizers', 'split', 'contractions', 'like', '``', 'do', \"n't\", \"''\", 'or', '``', 'it', \"'s\", \"''\", ',', 'while', 'others', 'do', \"n't\", '.']\n",
      "------------------------------\n",
      "Treebank Word Tokenization: \n",
      "['Hello', 'there', '!', 'I', \"'m\", 'testing', 'different', 'tokenizers—let', \"'s\", 'see', 'how', 'they', 'work.Some', 'tokenizers', 'split', 'contractions', 'like', '``', 'do', \"n't\", \"''\", 'or', '``', 'it', \"'s\", \"''\", ',', 'while', 'others', 'do', \"n't\", '.']\n",
      "------------------------------\n",
      "Regexp Tokenization: \n",
      "['Hello', 'there', 'I', 'm', 'testing', 'different', 'tokenizers', 'let', 's', 'see', 'how', 'they', 'work', 'Some', 'tokenizers', 'split', 'contractions', 'like', 'don', 't', 'or', 'it', 's', 'while', 'others', 'don', 't']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer, RegexpTokenizer\n",
    "\n",
    "print(\"Sentence Tokenization: \") \n",
    "print(sent_tokenize(corpus))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Word Tokenization: \")\n",
    "print(word_tokenize(corpus))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Treebank Word Tokenization: \")\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(corpus))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Regexp Tokenization: \")\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "print(tokenizer.tokenize(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecf70d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = \"Hello, world! I'm learning NLP. Tokenizers split sentences into words. Let's test: can't, won't, and it's. Wow! This is fun :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6098459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: \n",
      "['Hello, world!', \"I'm learning NLP.\", 'Tokenizers split sentences into words.', \"Let's test: can't, won't, and it's.\", 'Wow!', 'This is fun :)']\n",
      "------------------------------\n",
      "Word Tokenization: \n",
      "['Hello', ',', 'world', '!', 'I', \"'m\", 'learning', 'NLP', '.', 'Tokenizers', 'split', 'sentences', 'into', 'words', '.', 'Let', \"'s\", 'test', ':', 'ca', \"n't\", ',', 'wo', \"n't\", ',', 'and', 'it', \"'s\", '.', 'Wow', '!', 'This', 'is', 'fun', ':', ')']\n",
      "------------------------------\n",
      "Treebank Word Tokenization: \n",
      "['Hello', ',', 'world', '!', 'I', \"'m\", 'learning', 'NLP.', 'Tokenizers', 'split', 'sentences', 'into', 'words.', 'Let', \"'s\", 'test', ':', 'ca', \"n't\", ',', 'wo', \"n't\", ',', 'and', \"it's.\", 'Wow', '!', 'This', 'is', 'fun', ':', ')']\n",
      "------------------------------\n",
      "Regexp Tokenization: \n",
      "['Hello', 'world', 'I', 'm', 'learning', 'NLP', 'Tokenizers', 'split', 'sentences', 'into', 'words', 'Let', 's', 'test', 'can', 't', 'won', 't', 'and', 'it', 's', 'Wow', 'This', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Tokenization: \") \n",
    "print(sent_tokenize(corpus2))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Word Tokenization: \")\n",
    "print(word_tokenize(corpus2))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Treebank Word Tokenization: \")\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(corpus2))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Regexp Tokenization: \")\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "print(tokenizer.tokenize(corpus2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d97e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
